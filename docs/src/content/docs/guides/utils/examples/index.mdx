---
title: Utils Examples
description: Practical examples of using langcrew utilities
---

Practical examples demonstrating how to use langcrew utilities in real-world scenarios.

## File Detection Examples

### Document Processing Pipeline

```python
from langcrew.utils.file_detect import detect_file_type
import os

def process_documents(directory: str):
    """Process all documents in a directory"""
    results = []
    
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        
        # Detect file type and content
        file_info = detect_file_type(
            file_path, 
            read_content=True,
            max_content_size=5 * 1024 * 1024  # 5MB limit
        )
        
        # Process based on file type
        if file_info.is_text:
            result = {
                "filename": filename,
                "type": file_info.file_type,
                "size": file_info.size,
                "preview": file_info.content[:200] + "..." if len(file_info.content) > 200 else file_info.content,
                "word_count": len(file_info.content.split()) if file_info.content else 0
            }
        else:
            result = {
                "filename": filename,
                "type": file_info.file_type,
                "size": file_info.size,
                "binary": True
            }
        
        results.append(result)
    
    return results

# Usage
documents = process_documents("./documents")
for doc in documents:
    print(f"{doc['filename']}: {doc['type']} ({doc['size']} bytes)")
```

### Smart File Categorizer

```python
from langcrew.utils.file_detect import detect_file_type
from pathlib import Path

class FileCategorizer:
    def __init__(self):
        self.categories = {
            "documents": [".pdf", ".docx", ".txt", ".md", ".rtf"],
            "images": [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".svg"],
            "videos": [".mp4", ".avi", ".mov", ".wmv", ".flv"],
            "audio": [".mp3", ".wav", ".flac", ".ogg", ".m4a"],
            "archives": [".zip", ".rar", ".7z", ".tar", ".gz"],
            "code": [".py", ".js", ".html", ".css", ".json", ".xml"]
        }
    
    def categorize_file(self, file_path: str):
        """Categorize a file based on its type"""
        file_info = detect_file_type(file_path)
        
        for category, extensions in self.categories.items():
            if f".{file_info.file_type}" in extensions:
                return category
        
        return "other"
    
    def organize_directory(self, source_dir: str, target_dir: str):
        """Organize files by category"""
        source = Path(source_dir)
        target = Path(target_dir)
        
        for file_path in source.rglob("*"):
            if file_path.is_file():
                category = self.categorize_file(str(file_path))
                
                # Create category directory
                category_dir = target / category
                category_dir.mkdir(parents=True, exist_ok=True)
                
                # Move file
                new_path = category_dir / file_path.name
                print(f"Moving {file_path.name} to {category}/")

# Usage
categorizer = FileCategorizer()
categorizer.organize_directory("./downloads", "./organized")
```

## Token Counting Examples

### Cost Calculator

```python
from langcrew.utils.token_counter import TokenCounter

class CostCalculator:
    def __init__(self):
        self.model_prices = {
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-3.5-turbo": {"input": 0.002, "output": 0.002},
            "claude-3": {"input": 0.015, "output": 0.075}
        }
    
    def calculate_cost(self, text: str, model: str, response_length: int = 0):
        """Calculate the cost of processing text with a model"""
        counter = TokenCounter(model_name=model)
        input_tokens = counter.count_text(text)
        
        if model in self.model_prices:
            prices = self.model_prices[model]
            input_cost = (input_tokens / 1000) * prices["input"]
            output_cost = (response_length / 1000) * prices["output"]
            total_cost = input_cost + output_cost
            
            return {
                "model": model,
                "input_tokens": input_tokens,
                "output_tokens": response_length,
                "input_cost": input_cost,
                "output_cost": output_cost,
                "total_cost": total_cost
            }
        
        return None
    
    def compare_models(self, text: str, expected_response_tokens: int = 1000):
        """Compare costs across different models"""
        results = []
        
        for model in self.model_prices.keys():
            cost_info = self.calculate_cost(text, model, expected_response_tokens)
            if cost_info:
                results.append(cost_info)
        
        # Sort by total cost
        results.sort(key=lambda x: x["total_cost"])
        return results

# Usage
calculator = CostCalculator()
text = "Analyze this business proposal and provide recommendations..."

costs = calculator.compare_models(text, expected_response_tokens=2000)
for cost in costs:
    print(f"{cost['model']}: ${cost['total_cost']:.4f} ({cost['input_tokens']} input tokens)")
```

### Batch Processing Optimizer

```python
from langcrew.utils.token_counter import TokenCounter

class BatchOptimizer:
    def __init__(self, model: str, max_tokens: int = 4000):
        self.counter = TokenCounter(model_name=model)
        self.max_tokens = max_tokens
    
    def create_batches(self, texts: list[str], system_prompt: str = ""):
        """Create optimized batches that fit within token limits"""
        batches = []
        current_batch = []
        current_tokens = 0
        
        # Account for system prompt tokens
        system_tokens = self.counter.count_text(system_prompt) if system_prompt else 0
        
        for text in texts:
            text_tokens = self.counter.count_text(text)
            
            # Check if adding this text would exceed limit
            if current_tokens + text_tokens + system_tokens > self.max_tokens:
                if current_batch:
                    batches.append({
                        "texts": current_batch,
                        "total_tokens": current_tokens + system_tokens
                    })
                    current_batch = [text]
                    current_tokens = text_tokens
                else:
                    # Single text exceeds limit - split it
                    chunks = self._split_text(text, self.max_tokens - system_tokens)
                    for chunk in chunks:
                        batches.append({
                            "texts": [chunk],
                            "total_tokens": self.counter.count_text(chunk) + system_tokens
                        })
            else:
                current_batch.append(text)
                current_tokens += text_tokens
        
        # Add remaining batch
        if current_batch:
            batches.append({
                "texts": current_batch,
                "total_tokens": current_tokens + system_tokens
            })
        
        return batches
    
    def _split_text(self, text: str, max_tokens: int):
        """Split text into chunks that fit within token limit"""
        words = text.split()
        chunks = []
        current_chunk = []
        
        for word in words:
            test_chunk = " ".join(current_chunk + [word])
            if self.counter.count_text(test_chunk) > max_tokens:
                if current_chunk:
                    chunks.append(" ".join(current_chunk))
                    current_chunk = [word]
                else:
                    # Single word exceeds limit
                    chunks.append(word)
            else:
                current_chunk.append(word)
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks

# Usage
optimizer = BatchOptimizer("gpt-4", max_tokens=3000)
texts = ["Long text 1...", "Long text 2...", "Long text 3..."]

batches = optimizer.create_batches(texts, "You are a helpful assistant.")
for i, batch in enumerate(batches):
    print(f"Batch {i+1}: {len(batch['texts'])} texts, {batch['total_tokens']} tokens")
```

## Language Detection Examples

### Multi-language Content Router

```python
from langcrew.utils.language import detect_language
from langcrew.utils.message_utils import generate_message_id

class ContentRouter:
    def __init__(self):
        self.language_handlers = {
            "zh": self.handle_chinese,
            "en": self.handle_english,
            "fr": self.handle_french,
            "es": self.handle_spanish
        }
        self.default_handler = self.handle_english
    
    def route_content(self, content: str):
        """Route content to appropriate language handler"""
        language = detect_language(content)
        handler = self.language_handlers.get(language, self.default_handler)
        
        return {
            "id": generate_message_id(),
            "content": content,
            "detected_language": language,
            "handler": handler.__name__,
            "processed_content": handler(content)
        }
    
    def handle_chinese(self, content: str):
        """Handle Chinese content"""
        return f"处理中文内容: {content[:50]}..."
    
    def handle_english(self, content: str):
        """Handle English content"""
        return f"Processing English content: {content[:50]}..."
    
    def handle_french(self, content: str):
        """Handle French content"""
        return f"Traitement du contenu français: {content[:50]}..."
    
    def handle_spanish(self, content: str):
        """Handle Spanish content"""
        return f"Procesando contenido en español: {content[:50]}..."

# Usage
router = ContentRouter()

contents = [
    "Hello, how are you today?",
    "你好，今天怎么样？",
    "Bonjour, comment allez-vous?",
    "Hola, ¿cómo estás?"
]

for content in contents:
    result = router.route_content(content)
    print(f"Language: {result['detected_language']}, Handler: {result['handler']}")
```

### Content Analysis Pipeline

```python
from langcrew.utils.file_detect import detect_file_type
from langcrew.utils.language import detect_language
from langcrew.utils.token_counter import TokenCounter

class ContentAnalyzer:
    def __init__(self):
        self.counter = TokenCounter(model_name="gpt-4")
    
    def analyze_file(self, file_path: str):
        """Comprehensive file analysis"""
        # Step 1: File detection
        file_info = detect_file_type(file_path, read_content=True)
        
        analysis = {
            "file_path": file_path,
            "file_type": file_info.file_type,
            "file_size": file_info.size,
            "is_text": file_info.is_text,
            "mime_type": file_info.mime_type
        }
        
        if file_info.is_text and file_info.content:
            # Step 2: Language detection
            language = detect_language(file_info.content)
            
            # Step 3: Token counting
            tokens = self.counter.count_text(file_info.content)
            
            # Step 4: Content statistics
            analysis.update({
                "language": language,
                "tokens": tokens,
                "characters": len(file_info.content),
                "words": len(file_info.content.split()),
                "lines": file_info.content.count('\n') + 1,
                "content_preview": file_info.content[:200] + "..." if len(file_info.content) > 200 else file_info.content
            })
        
        return analysis
    
    def analyze_directory(self, directory_path: str):
        """Analyze all files in a directory"""
        import os
        results = []
        
        for root, dirs, files in os.walk(directory_path):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    analysis = self.analyze_file(file_path)
                    results.append(analysis)
                except Exception as e:
                    results.append({
                        "file_path": file_path,
                        "error": str(e)
                    })
        
        return results
    
    def generate_report(self, analyses: list):
        """Generate summary report"""
        total_files = len(analyses)
        text_files = sum(1 for a in analyses if a.get("is_text", False))
        
        languages = {}
        total_tokens = 0
        
        for analysis in analyses:
            if "language" in analysis:
                lang = analysis["language"]
                languages[lang] = languages.get(lang, 0) + 1
                total_tokens += analysis.get("tokens", 0)
        
        return {
            "total_files": total_files,
            "text_files": text_files,
            "binary_files": total_files - text_files,
            "languages": languages,
            "total_tokens": total_tokens,
            "average_tokens": total_tokens / text_files if text_files > 0 else 0
        }

# Usage
analyzer = ContentAnalyzer()

# Analyze single file
file_analysis = analyzer.analyze_file("document.txt")
print(f"File: {file_analysis['file_type']}, Language: {file_analysis.get('language', 'N/A')}")

# Analyze directory
directory_analyses = analyzer.analyze_directory("./documents")
report = analyzer.generate_report(directory_analyses)
print(f"Analyzed {report['total_files']} files, {report['text_files']} text files")
print(f"Languages found: {list(report['languages'].keys())}")
```

## Message Utilities Examples

### Message Tracking System

```python
from langcrew.utils.message_utils import generate_message_id
import time
import json

class MessageTracker:
    def __init__(self):
        self.messages = {}
        self.conversations = {}
    
    def create_message(self, content: str, role: str, conversation_id: str = None):
        """Create a tracked message"""
        message_id = generate_message_id()
        timestamp = time.time()
        
        message = {
            "id": message_id,
            "content": content,
            "role": role,
            "timestamp": timestamp,
            "conversation_id": conversation_id
        }
        
        # Store message
        self.messages[message_id] = message
        
        # Add to conversation
        if conversation_id:
            if conversation_id not in self.conversations:
                self.conversations[conversation_id] = []
            self.conversations[conversation_id].append(message_id)
        
        return message
    
    def get_conversation(self, conversation_id: str):
        """Get all messages in a conversation"""
        if conversation_id not in self.conversations:
            return []
        
        message_ids = self.conversations[conversation_id]
        return [self.messages[msg_id] for msg_id in message_ids]
    
    def export_conversation(self, conversation_id: str, format: str = "json"):
        """Export conversation in specified format"""
        messages = self.get_conversation(conversation_id)
        
        if format == "json":
            return json.dumps(messages, indent=2)
        elif format == "markdown":
            md_content = f"# Conversation {conversation_id}\n\n"
            for msg in messages:
                md_content += f"**{msg['role'].title()}**: {msg['content']}\n\n"
            return md_content
        else:
            return str(messages)

# Usage
tracker = MessageTracker()

# Create conversation
conv_id = "conv_001"
tracker.create_message("Hello, how can I help you?", "assistant", conv_id)
tracker.create_message("I need help with Python programming", "user", conv_id)
tracker.create_message("I'd be happy to help with Python!", "assistant", conv_id)

# Export conversation
markdown_export = tracker.export_conversation(conv_id, "markdown")
print(markdown_export)
```

## Next Steps

- **[Utils Configuration](/guides/utils/configuration)** - Advanced configuration options
- **[Utils Concepts](/concepts/utils)** - Understanding the architecture
- **Examples** - Check `examples/components/utils/` for more implementations