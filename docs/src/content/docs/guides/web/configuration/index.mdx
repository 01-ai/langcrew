---
title: Web Service Configuration
description: Complete configuration guide for Web services
---

Comprehensive configuration guide for Web services to meet your needs.

## Basic Configuration

### Simple HTTP Server

For basic API endpoints:

```python
from langcrew.web import create_server

# Basic server configuration
server = create_server(
    host="0.0.0.0",
    port=8000,
    cors_origins=["http://localhost:3000"],
    debug=False
)
```

### LangGraph Server

For advanced workflow integration:

```python
from langcrew.web import create_langgraph_server

# LangGraph server configuration
server = create_langgraph_server(
    crew=crew,
    host="0.0.0.0",
    port=8000,
    enable_streaming=True,
    tool_display=True,
    cors_origins=["*"]
)
```

## Advanced Configuration

### AdapterServer with Custom Settings

For complete control over server behavior:

```python
from langcrew.web import AdapterServer
from langcrew.web.langgraph_adapter import LangGraphAdapter

# Custom adapter configuration
adapter = LangGraphAdapter(
    crew=crew,
    enable_streaming=True,
    tool_display=True,
    session_timeout=3600,  # 1 hour
    max_concurrent_sessions=100,
    memory_config={
        "provider": "mem0",
        "config": {
            "vector_store": {
                "provider": "chroma",
                "config": {"path": "./memory_db"}
            }
        }
    }
)

# Custom server configuration
server = AdapterServer(
    adapter=adapter,
    host="0.0.0.0",
    port=8000,
    cors_origins=["http://localhost:3000", "https://yourdomain.com"],
    cors_methods=["GET", "POST", "OPTIONS"],
    cors_headers=["*"],
    max_request_size=10 * 1024 * 1024,  # 10MB
    request_timeout=300,  # 5 minutes
    debug=False,
    log_level="INFO"
)
```

## Configuration Options

### Server Configuration

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `host` | str | "127.0.0.1" | Server host address |
| `port` | int | 8000 | Server port number |
| `cors_origins` | List[str] | ["*"] | Allowed CORS origins |
| `cors_methods` | List[str] | ["*"] | Allowed CORS methods |
| `cors_headers` | List[str] | ["*"] | Allowed CORS headers |
| `debug` | bool | False | Enable debug mode |
| `log_level` | str | "INFO" | Logging level |
| `max_request_size` | int | 16MB | Maximum request size |
| `request_timeout` | int | 60 | Request timeout in seconds |

### Adapter Configuration

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `enable_streaming` | bool | True | Enable real-time streaming |
| `tool_display` | bool | True | Show tool usage in UI |
| `session_timeout` | int | 1800 | Session timeout in seconds |
| `max_concurrent_sessions` | int | 50 | Maximum concurrent sessions |
| `memory_config` | dict | None | Memory system configuration |

### Memory Configuration

```python
memory_config = {
    "provider": "mem0",  # or "langchain"
    "config": {
        "vector_store": {
            "provider": "chroma",  # or "qdrant", "weaviate"
            "config": {
                "path": "./memory_db",
                "collection_name": "langcrew_memory"
            }
        },
        "llm": {
            "provider": "openai",
            "config": {
                "model": "gpt-4o-mini",
                "temperature": 0.1
            }
        }
    }
}
```

## Environment Configuration

### Environment Variables

Set these environment variables for production deployment:

```bash
# Server Configuration
LANGCREW_HOST=0.0.0.0
LANGCREW_PORT=8000
LANGCREW_DEBUG=false
LANGCREW_LOG_LEVEL=INFO

# CORS Configuration
LANGCREW_CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com
LANGCREW_CORS_METHODS=GET,POST,OPTIONS
LANGCREW_CORS_HEADERS=*

# Security Configuration
LANGCREW_MAX_REQUEST_SIZE=16777216  # 16MB
LANGCREW_REQUEST_TIMEOUT=60

# Memory Configuration
LANGCREW_MEMORY_PROVIDER=mem0
LANGCREW_MEMORY_VECTOR_STORE=chroma
LANGCREW_MEMORY_DB_PATH=./memory_db

# LLM Configuration
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-4o-mini
```

### Configuration File

Create a `langcrew.yaml` configuration file:

```yaml
server:
  host: "0.0.0.0"
  port: 8000
  debug: false
  log_level: "INFO"
  cors:
    origins: 
      - "https://yourdomain.com"
      - "https://app.yourdomain.com"
    methods: ["GET", "POST", "OPTIONS"]
    headers: ["*"]
  limits:
    max_request_size: 16777216  # 16MB
    request_timeout: 60

adapter:
  enable_streaming: true
  tool_display: true
  session_timeout: 1800
  max_concurrent_sessions: 50
  
memory:
  provider: "mem0"
  config:
    vector_store:
      provider: "chroma"
      config:
        path: "./memory_db"
        collection_name: "langcrew_memory"
    llm:
      provider: "openai"
      config:
        model: "gpt-4o-mini"
        temperature: 0.1

llm:
  default:
    provider: "openai"
    model: "gpt-4o-mini"
    temperature: 0
    max_tokens: 4000
```

Load configuration from file:

```python
from langcrew.web import create_server_from_config

# Load server from configuration file
server = create_server_from_config("langcrew.yaml", crew=crew)
server.run()
```

## Production Configuration

### Docker Configuration

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application
COPY . .

# Set environment variables
ENV LANGCREW_HOST=0.0.0.0
ENV LANGCREW_PORT=8000
ENV LANGCREW_DEBUG=false

# Expose port
EXPOSE 8000

# Run server
CMD ["python", "-m", "langcrew.web", "--config", "langcrew.yaml"]
```

### Docker Compose

```yaml
version: '3.8'

services:
  langcrew-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGCREW_HOST=0.0.0.0
      - LANGCREW_PORT=8000
      - LANGCREW_DEBUG=false
      - LANGCREW_MEMORY_DB_PATH=/app/data/memory_db
    volumes:
      - ./data:/app/data
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - langcrew-api
    restart: unless-stopped
```

### Nginx Configuration

```nginx
upstream langcrew_backend {
    server langcrew-api:8000;
}

server {
    listen 80;
    server_name yourdomain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name yourdomain.com;

    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;

    location / {
        proxy_pass http://langcrew_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocket support
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
}
```

## Security Configuration

### API Key Authentication

```python
from langcrew.web import create_server

server = create_server(
    crew=crew,
    auth_config={
        "type": "api_key",
        "api_keys": ["your-secret-api-key"],
        "header_name": "X-API-Key"
    }
)
```

### JWT Authentication

```python
server = create_server(
    crew=crew,
    auth_config={
        "type": "jwt",
        "secret_key": "your-jwt-secret",
        "algorithm": "HS256",
        "token_expiry": 3600  # 1 hour
    }
)
```

### Rate Limiting

```python
server = create_server(
    crew=crew,
    rate_limit_config={
        "requests_per_minute": 60,
        "requests_per_hour": 1000,
        "burst_size": 10
    }
)
```

## Monitoring Configuration

### Health Checks

```python
server = create_server(
    crew=crew,
    health_check_config={
        "enabled": True,
        "endpoint": "/health",
        "include_memory": True,
        "include_llm": True
    }
)
```

### Metrics

```python
server = create_server(
    crew=crew,
    metrics_config={
        "enabled": True,
        "endpoint": "/metrics",
        "include_system_metrics": True,
        "include_business_metrics": True
    }
)
```

## Troubleshooting

### Common Issues

1. **CORS Errors**: Ensure your frontend domain is in `cors_origins`
2. **Memory Issues**: Configure appropriate `session_timeout` and `max_concurrent_sessions`
3. **Performance Issues**: Adjust `max_request_size` and `request_timeout`
4. **Connection Issues**: Check firewall settings and port availability

### Debug Mode

Enable debug mode for development:

```python
server = create_server(
    crew=crew,
    debug=True,
    log_level="DEBUG"
)
```

This will provide detailed logging and error information.