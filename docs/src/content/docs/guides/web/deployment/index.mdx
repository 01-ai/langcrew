---
title: Web Deployment
description: Deploy your AI agents as production web services
---

Complete guide to deploying langcrew web services in production environments.

## Production Deployment

### Basic Production Setup

Deploy a simple production server:

```python
from langcrew import Agent, Crew
from langcrew.web import create_langgraph_server

# Create production-ready crew
agent = Agent(
    role="Production Assistant",
    goal="Handle user requests reliably",
    backstory="Production-grade AI assistant"
)

crew = Crew(agents=[agent])

# Production server configuration
server = create_langgraph_server(
    crew=crew,
    host="0.0.0.0",
    port=8000,
    cors_origins=["https://yourdomain.com"],
    debug=False,
    workers=4
)

server.run()
```

### Docker Deployment

Create a Dockerfile for containerized deployment:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application
COPY . .

# Expose port
EXPOSE 8000

# Start server
CMD ["python", "app.py"]
```

Build and run:

```bash
# Build image
docker build -t langcrew-web .

# Run container
docker run -p 8000:8000 \
  -e OPENAI_API_KEY=your_key_here \
  langcrew-web
```

### Kubernetes Deployment

Deploy to Kubernetes cluster:

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langcrew-web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langcrew-web
  template:
    metadata:
      labels:
        app: langcrew-web
    spec:
      containers:
      - name: langcrew-web
        image: langcrew-web:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: langcrew-secrets
              key: openai-api-key
---
apiVersion: v1
kind: Service
metadata:
  name: langcrew-web-service
spec:
  selector:
    app: langcrew-web
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

## Frontend UI Integration

### Setting Up the Frontend

The Web component comes with a complete React-based frontend UI:

```bash
# Navigate to frontend directory
cd web

# Install dependencies
pnpm install

# Configure backend URL
echo "VITE_API_URL=http://localhost:8000" > .env.local

# Start development server
pnpm dev
```

### Production Frontend Build

```bash
# Build for production
pnpm build

# Serve with nginx or your preferred server
nginx -s reload
```

### Frontend Configuration

Configure the frontend to connect to your backend:

```javascript
// web/src/config/api.js
export const API_CONFIG = {
  baseURL: process.env.VITE_API_URL || 'http://localhost:8000',
  timeout: 30000,
  enableStreaming: true
};
```

## Load Balancing

### Nginx Configuration

```nginx
upstream langcrew_backend {
    server 127.0.0.1:8000;
    server 127.0.0.1:8001;
    server 127.0.0.1:8002;
}

server {
    listen 80;
    server_name yourdomain.com;

    location / {
        proxy_pass http://langcrew_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # WebSocket support
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

### Multiple Server Instances

Run multiple server instances:

```python
# server1.py (port 8000)
# server2.py (port 8001) 
# server3.py (port 8002)

from langcrew.web import create_langgraph_server

def create_server_instance(port: int):
    server = create_langgraph_server(
        crew=crew,
        host="0.0.0.0",
        port=port,
        workers=2
    )
    return server

if __name__ == "__main__":
    import sys
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000
    server = create_server_instance(port)
    server.run()
```

## Monitoring and Logging

### Health Checks

Add health check endpoints:

```python
@server.app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "version": "1.0.0"
    }

@server.app.get("/ready")
async def readiness_check():
    # Check database connections, etc.
    return {"status": "ready"}
```

### Structured Logging

Configure production logging:

```python
import logging
import json
from datetime import datetime

class ProductionLogger:
    def __init__(self):
        self.logger = logging.getLogger("langcrew.production")
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        ))
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def log_request(self, request, response, duration):
        self.logger.info(json.dumps({
            "type": "request",
            "method": request.method,
            "url": str(request.url),
            "status_code": response.status_code,
            "duration_ms": duration * 1000,
            "timestamp": datetime.utcnow().isoformat()
        }))
```

## Security

### HTTPS Configuration

```python
# SSL configuration
server = create_langgraph_server(
    crew=crew,
    host="0.0.0.0",
    port=443,
    ssl_keyfile="/path/to/private.key",
    ssl_certfile="/path/to/certificate.crt"
)
```

### Authentication Middleware

```python
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer

security = HTTPBearer()

async def verify_token(credentials = Depends(security)):
    token = credentials.credentials
    # Verify token with your auth service
    if not is_valid_token(token):
        raise HTTPException(
            status_code=401, 
            detail="Invalid authentication token"
        )
    return token

# Apply to routes
@server.app.post("/chat")
async def chat_endpoint(token = Depends(verify_token)):
    # Protected endpoint
    pass
```

### Rate Limiting

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
server.app.state.limiter = limiter

@server.app.post("/chat")
@limiter.limit("10/minute")
async def chat_endpoint(request: Request):
    # Rate-limited endpoint
    pass
```

## Performance Optimization

### Connection Pooling

```python
# Database connection pooling
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "postgresql://user:pass@host/db",
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_timeout=30
)
```

### Caching

```python
from functools import lru_cache
import redis

# Redis caching
redis_client = redis.Redis(host='localhost', port=6379, db=0)

@lru_cache(maxsize=1000)
def cached_function(input_data):
    # Expensive computation
    return result
```

## Troubleshooting

### Common Issues

**High Memory Usage**:
```python
# Limit concurrent requests
server = create_langgraph_server(
    crew=crew,
    max_concurrent_requests=50
)
```

**Slow Response Times**:
```python
# Optimize streaming
adapter = LangGraphAdapter(
    crew=crew,
    enable_streaming=True,
    buffer_size=512,
    flush_interval=0.1
)
```

**Connection Timeouts**:
```python
# Adjust timeouts
server = create_langgraph_server(
    crew=crew,
    request_timeout=60,
    keep_alive_timeout=5
)
```

## Next Steps

- **[Web Configuration](/guides/web/configuration)** - Advanced configuration options
- **[Web Concepts](/concepts/web)** - Understanding the architecture
- **Examples** - Check `examples/components/web/` for deployment examples