---
title: 工具库配置
description: 工具库的完整配置指南
---

工具库的全面配置指南，满足您的需求。

## 基础配置

### 文件检测配置

配置文件检测行为：

```python
from langcrew.utils.file_detect import detect_file_type

# 基本文件检测
file_info = detect_file_type(
    file_path="document.pdf",
    read_content=True,
    extract_metadata=True
)

# 配置内容读取限制
file_info = detect_file_type(
    file_path="large_file.txt",
    read_content=True,
    max_content_size=1024 * 1024,  # 1MB限制
    encoding="utf-8"
)
```

### 令牌计数配置

为不同模型配置令牌计数：

```python
from langcrew.utils.token_counter import TokenCounter

# 基本令牌计数
counter = TokenCounter(model_name="gpt-4")
tokens = counter.count_text("你好，世界！")

# 带元数据的高级令牌计数
counter = TokenCounter(model_name="gpt-4")
result = counter.count_text("你好，世界！")

print(result)  # 令牌数量
```

### 语言检测配置

配置语言检测参数：

```python
from langcrew.utils.language import detect_language, detect_chinese

# 基本语言检测
language = detect_language("你好，世界！")

# 中文检测
is_chinese = detect_chinese(
    text="你好，comment allez-vous?",
)

print(f"是中文: {is_chinese}")  # True（如果>30%中文字符）
```

## 高级配置

### 自定义文件类型处理器

注册自定义文件类型处理器：

```python
from langcrew.utils.file_detect import FileDetector

# 创建自定义检测器
detector = FileDetector()

# 注册自定义处理器
@detector.register_handler(".custom")
def handle_custom_file(file_path: str):
    return {
        "file_type": "custom",
        "mime_type": "application/x-custom",
        "is_text": True
    }

# 使用自定义检测器
file_info = detector.detect(file_path="data.custom")
```

### 自定义分词器

配置自定义分词器：

```python
from langcrew.utils.token_counter import TokenCounter
import tiktoken

# 创建自定义令牌计数器
counter = TokenCounter(model_name="custom-gpt")

# 注册自定义模型
counter.register_model(
    model_name="custom-gpt",
    encoding=tiktoken.get_encoding("cl100k_base")
)

# 使用自定义模型
tokens = counter.count_text("你好", model="custom-gpt")
```

### 多语言配置

为特定用例配置语言检测：

```python
from langcrew.utils.language import LanguageDetector

# 创建支持特定语言的检测器
detector = LanguageDetector(
    supported_languages=["zh", "en", "fr", "es", "de"],
    min_text_length=10,
    confidence_threshold=0.7
)

# 使用配置的检测器
result = detector.detect("Hola, ¿cómo estás?")
```

## 配置模式

### 模式 1：文档处理管道

用于文档分析工作流：

```python
from langcrew.utils.file_detect import detect_file_type
from langcrew.utils.token_counter import TokenCounter
from langcrew.utils.language import detect_language

class DocumentProcessor:
    def __init__(self):
        self.max_file_size = 10 * 1024 * 1024  # 10MB
        self.supported_types = [".txt", ".pdf", ".docx", ".md"]
        self.token_limit = 4000
    
    def process(self, file_path: str):
        # 步骤1：检测文件类型
        file_info = detect_file_type(
            file_path,
            read_content=True,
            max_content_size=self.max_file_size
        )
        
        if file_info.file_type not in self.supported_types:
            raise ValueError(f"不支持的文件类型: {file_info.file_type}")
        
        # 步骤2：检查令牌数量
        if file_info.content:
            counter = TokenCounter(model_name="gpt-4")
            tokens = counter.count_text(file_info.content)
            if tokens > self.token_limit:
                # 截断内容
                file_info.content = self.truncate_content(
                    file_info.content, self.token_limit
                )
        
        # 步骤3：检测语言
        language = detect_language(file_info.content)
        
        return {
            "file_info": file_info,
            "tokens": tokens,
            "language": language
        }
```

### 模式 2：多模型令牌优化

用于跨模型的成本优化：

```python
from langcrew.utils.token_counter import TokenCounter

class TokenOptimizer:
    def __init__(self):
        self.model_costs = {
            "gpt-4": 0.03,      # 每1K令牌
            "gpt-3.5-turbo": 0.002,
            "claude-3": 0.015
        }
    
    def find_optimal_model(self, text: str, max_cost: float):
        results = []
        
        for model, cost_per_1k in self.model_costs.items():
            counter = TokenCounter(model_name=model)
            tokens = counter.count_text(text)
            total_cost = (tokens / 1000) * cost_per_1k
            
            results.append({
                "model": model,
                "tokens": tokens,
                "cost": total_cost
            })
        
        # 按成本筛选并返回最便宜的
        affordable = [r for r in results if r["cost"] <= max_cost]
        return min(affordable, key=lambda x: x["cost"]) if affordable else None
```

### 模式 3：智能消息路由

用于多语言应用：

```python
from langcrew.utils.language import detect_language
from langcrew.utils.message_utils import generate_message_id

class MessageRouter:
    def __init__(self):
        self.language_agents = {
            "zh": "chinese_agent",
            "en": "english_agent", 
            "fr": "french_agent",
            "es": "spanish_agent"
        }
        self.default_agent = "english_agent"
    
    def route_message(self, content: str):
        # 生成唯一ID
        msg_id = generate_message_id()
        
        # 检测语言
        language = detect_language(content)
        
        # 选择合适的智能体
        agent = self.language_agents.get(language, self.default_agent)
        
        return {
            "message_id": msg_id,
            "content": content,
            "language": language,
            "assigned_agent": agent
        }
```

### 模式 4：内容分析管道

用于全面的内容分析：

```python
from langcrew.utils.file_detect import detect_file_type
from langcrew.utils.token_counter import TokenCounter
from langcrew.utils.language import detect_language

class ContentAnalyzer:
    def __init__(self):
        self.analysis_config = {
            "extract_metadata": True,
            "count_tokens": True,
            "detect_language": True,
            "generate_summary": True
        }
    
    def analyze(self, file_path: str):
        results = {}
        
        # 文件分析
        if self.analysis_config["extract_metadata"]:
            file_info = detect_file_type(file_path, read_content=True)
            results["file_info"] = {
                "type": file_info.file_type,
                "size": file_info.size,
                "mime_type": file_info.mime_type
            }
        
        # 令牌分析
        if self.analysis_config["count_tokens"] and file_info.content:
            counter = TokenCounter(model_name="gpt-4")
            tokens = counter.count_text(file_info.content)
            results["tokens"] = {
                "count": tokens,
                "model": "gpt-4"
            }
        
        # 语言分析
        if self.analysis_config["detect_language"] and file_info.content:
            language = detect_language(file_info.content)
            results["language"] = {
                "code": language
            }
        
        return results
```

## 工具配置选项

### 文件检测选项

| 参数 | 类型 | 描述 | 默认值 |
|------|------|------|--------|
| `read_content` | bool | 读取文件内容 | True |
| `extract_metadata` | bool | 提取文件元数据 | True |
| `max_content_size` | int | 最大内容大小（字节） | 10MB |
| `encoding` | str | 文本编码 | "utf-8" |
| `binary_threshold` | float | 二进制检测阈值 | 0.3 |

### 令牌计数器选项

| 参数 | 类型 | 描述 | 默认值 |
|------|------|------|--------|
| `model_name` | str | 分词模型 | "gpt-4" |
| `cache_encodings` | bool | 缓存分词器编码 | True |
| `max_cache_size` | int | 最大编码缓存大小 | 100 |

### 语言检测选项

| 参数 | 类型 | 描述 | 默认值 |
|------|------|------|--------|
| `min_text_length` | int | 最小文本长度 | 3 |
| `supported_languages` | List[str] | 支持的语言代码 | 全部 |

## 性能调优

### 优化文件处理

```python
# 对于大文件
file_info = detect_file_type(
    file_path="large_file.txt",
    read_content=False,  # 跳过内容读取
    extract_metadata=False  # 跳过元数据提取
)

# 对于批量处理
files = ["file1.txt", "file2.pdf", "file3.docx"]
results = []

for file_path in files:
    info = detect_file_type(
        file_path,
        read_content=True,
        max_content_size=1024 * 1024  # 1MB限制
    )
    results.append(info)
```

### 优化令牌计数

```python
# 为重复使用缓存分词器
from langcrew.utils.token_counter import TokenCounter

counter = TokenCounter(model_name="gpt-4", cache_encodings=True)

# 批量令牌计数
texts = ["你好", "世界", "你好吗？"]
token_counts = [counter.count_text(text) for text in texts]
```

## 最佳实践

- **文件大小限制**：为内容读取设置适当限制
- **模型选择**：为目标模型选择正确的分词器
- **语言检测**：为准确检测提供足够的文本
- **缓存**：为重复操作启用缓存
- **错误处理**：优雅处理文件访问和处理错误
- **内存管理**：可能时分块处理大文件

## 下一步

- **[工具库概念](/zh/concepts/utils)** - 理解架构
- **[快速开始](/zh/guides/utils/getting-started)** - 快速开始指南
- **示例** - 查看 `examples/components/utils/` 了解实现案例
- **[社区论坛](https://github.com/01-ai/langcrew/discussions)** - 获得帮助