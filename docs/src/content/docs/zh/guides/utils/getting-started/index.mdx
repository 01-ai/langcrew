---
title: å·¥å…·åº“ - å¿«é€Ÿå¼€å§‹
description: 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹å·¥å…·åº“
---

å¿«é€ŸæŒ‡å—ï¼Œè®©å·¥å…·åº“åœ¨æ‚¨çš„langcrewåº”ç”¨ä¸­å·¥ä½œã€‚

## å®‰è£…

å·¥å…·åº“åŒ…å«åœ¨langcrewä¸­ - æ— éœ€é¢å¤–å®‰è£…ï¼š

```bash
uv add langcrew --prerelease=allow --index https://nexus.lingyiwanwu.net/repository/pypi-hosted/simple
```

## åŸºç¡€ç¤ºä¾‹

### æ–‡ä»¶æ£€æµ‹

è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶æå–å…ƒæ•°æ®ï¼š

```python
from langcrew.utils.file_detect import detect_file_type

# æ£€æµ‹å„ç§æ–‡ä»¶ç±»å‹
pdf_info = detect_file_type("document.pdf")
print(f"ç±»å‹: {pdf_info.file_type}")
print(f"MIME: {pdf_info.mime_type}")
print(f"å¤§å°: {pdf_info.size} å­—èŠ‚")

# æ£€æµ‹å¸¦å†…å®¹çš„æ–‡æœ¬æ–‡ä»¶
text_info = detect_file_type("README.md", read_content=True)
print(f"å†…å®¹é¢„è§ˆ: {text_info.content[:100]}...")
print(f"æ˜¯æ–‡æœ¬æ–‡ä»¶: {text_info.is_text}")
```

### ä»¤ç‰Œè®¡æ•°

ä¸ºä¸åŒæ¨¡å‹å‡†ç¡®è®¡ç®—ä»¤ç‰Œï¼š

```python
from langcrew.utils.token_counter import TokenCounter

# ä¸ºç‰¹å®šæ¨¡å‹åˆ›å»ºä»¤ç‰Œè®¡æ•°å™¨
counter = TokenCounter(model_name="gpt-4")

# è®¡ç®—æ–‡æœ¬ä¸­çš„ä»¤ç‰Œ
tokens = counter.count_text("ä½ å¥½ï¼Œä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ")
print(f"GPT-4 ä»¤ç‰Œ: {tokens}")

# åŸºäºæ¶ˆæ¯çš„ä»¤ç‰Œè®¡æ•°ï¼ˆå¸¦ç²¾ç¡®/è¿‘ä¼¼æŒ‡ç¤ºå™¨ï¼‰
messages = [
    {"role": "system", "content": "æ‚¨æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "æ³•å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"},
    {"role": "assistant", "content": "æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»ã€‚"}
]

token_count, is_exact = counter.count_messages(messages)
print(f"æ€»å¯¹è¯ä»¤ç‰Œ: {token_count} ({'ç²¾ç¡®' if is_exact else 'è¿‘ä¼¼'})")
```

### è¯­è¨€æ£€æµ‹

æ£€æµ‹è¯­è¨€å¹¶å¤„ç†æ–‡æœ¬ï¼š

```python
from langcrew.utils.language import detect_language, detect_chinese

# æ£€æµ‹è¯­è¨€
english_text = "Hello, how are you doing today?"
chinese_text = "ä½ å¥½ï¼Œä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ"
mixed_text = "Hello, ä¸–ç•Œï¼"

print(f"è‹±æ–‡: {detect_language(english_text)}")  # "en"
print(f"ä¸­æ–‡: {detect_language(chinese_text)}")    # "zh"

# ä¸­æ–‡æ£€æµ‹
is_chinese = detect_chinese(mixed_text)
print(f"åŒ…å«ä¸­æ–‡: {is_chinese}")  # Trueï¼ˆå¦‚æœ>30%ä¸­æ–‡å­—ç¬¦ï¼‰
```

### æ¶ˆæ¯å·¥å…·

ç”ŸæˆIDå’Œæ ¼å¼åŒ–æ¶ˆæ¯ï¼š

```python
from langcrew.utils.message_utils import generate_message_id

# ç”Ÿæˆå”¯ä¸€æ¶ˆæ¯ID
msg_id = generate_message_id()
print(f"æ¶ˆæ¯ID: {msg_id}")

# åœ¨Webåº”ç”¨ä¸­ä½¿ç”¨æ¶ˆæ¯è·Ÿè¸ª
def create_message(content: str, role: str):
    return {
        "id": generate_message_id(),
        "content": content,
        "role": role,
        "timestamp": time.time()
    }

message = create_message("ä½ å¥½ï¼Œä¸–ç•Œï¼", "user")
print(message)
```

## é›†æˆç¤ºä¾‹

åœ¨æ™ºèƒ½ä½“å·¥ä½œæµä¸­ä½¿ç”¨å·¥å…·ï¼š

```python
from langcrew import Agent, Task
from langcrew.utils.file_detect import detect_file_type
from langcrew.utils.token_counter import TokenCounter
from langcrew.utils.language import detect_language

# åˆ›å»ºæ–‡æ¡£åˆ†ææ™ºèƒ½ä½“
analyzer = Agent(
    role="æ–‡æ¡£åˆ†æå¸ˆ",
    goal="åˆ†ææ–‡æ¡£å¹¶æä¾›è§è§£",
    backstory="å¤„ç†å„ç§æ–‡æ¡£ç±»å‹çš„ä¸“å®¶"
)

def analyze_document(file_path: str):
    """å…¨é¢çš„æ–‡æ¡£åˆ†æ"""
    
    # æ­¥éª¤1ï¼šæ£€æµ‹æ–‡ä»¶ç±»å‹
    file_info = detect_file_type(file_path, read_content=True)
    print(f"æ–‡ä»¶ç±»å‹: {file_info.file_type}")
    
    # æ­¥éª¤2ï¼šå¦‚æœæ˜¯æ–‡æœ¬åˆ™è®¡ç®—ä»¤ç‰Œ
    if file_info.is_text and file_info.content:
        counter = TokenCounter(model_name="gpt-4")
        tokens = counter.count_text(file_info.content)
        print(f"ä»¤ç‰Œæ•°é‡: {tokens}")
        
        # æ­¥éª¤3ï¼šæ£€æµ‹è¯­è¨€
        language = detect_language(file_info.content)
        print(f"è¯­è¨€: {language}")
        
        # æ­¥éª¤4ï¼šåˆ›å»ºåˆ†æä»»åŠ¡
        task = Task(
            agent=analyzer,
            description=f"åˆ†æè¿™ä¸ª{language}æ–‡æ¡£ï¼ŒåŒ…å«{tokens}ä¸ªä»¤ç‰Œ",
            expected_output="æ–‡æ¡£æ‘˜è¦å’Œå…³é”®è§è§£"
        )
        
        return task.execute()
    
    return f"æ— æ³•åˆ†æ{file_info.file_type}æ–‡ä»¶"

# æµ‹è¯•å‡½æ•°
result = analyze_document("sample.txt")
print(result)
```

## æµ‹è¯•æ‚¨çš„è®¾ç½®

### è¿è¡Œå®Œæ•´ç¤ºä¾‹

```bash
# è®¾ç½®æ‚¨çš„ OpenAI API å¯†é’¥
export OPENAI_API_KEY=your_api_key_here

# ä»å·¥å…·åº“ç¤ºä¾‹ç›®å½•è¿è¡Œ
cd examples/components/utils
uv run utils_example.py
```

è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†ï¼š
- å„ç§æ ¼å¼çš„æ–‡ä»¶ç±»å‹æ£€æµ‹
- ä¸åŒæ¨¡å‹çš„ä»¤ç‰Œè®¡æ•°
- å¤šè¯­è¨€å†…å®¹çš„è¯­è¨€æ£€æµ‹
- æ¶ˆæ¯æ ¼å¼åŒ–å’ŒIDç”Ÿæˆ

## æ›´å¤šç¤ºä¾‹

è¦äº†è§£æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ï¼š

- **ğŸ“ `examples/components/utils/`** - å®Œæ•´çš„å·¥å…·å®ç°
- **ğŸ“„ `examples/components/utils/README.md`** - è®¾ç½®è¯´æ˜
- **ğŸ `examples/components/utils/utils_example.py`** - å¯è¿è¡Œä»£ç 
- **ğŸ“Š `examples/components/utils/token_analysis.py`** - ä»¤ç‰Œè®¡æ•°ç¤ºä¾‹

## æ•…éšœæ’é™¤

### æ–‡ä»¶æ£€æµ‹é—®é¢˜

ç¡®ä¿æ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»ï¼š
```python
import os

# âŒ é”™è¯¯ - æ–‡ä»¶ä¸å­˜åœ¨
file_info = detect_file_type("nonexistent.txt")

# âœ… æ­£ç¡® - æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if os.path.exists("document.pdf"):
    file_info = detect_file_type("document.pdf")
else:
    print("æ–‡ä»¶æœªæ‰¾åˆ°")
```

### ä»¤ç‰Œè®¡æ•°é”™è¯¯

ä½¿ç”¨æ”¯æŒçš„æ¨¡å‹åç§°ï¼š
```python
# âŒ é”™è¯¯ - ä¸æ”¯æŒçš„æ¨¡å‹
counter = TokenCounter(model_name="unsupported-model")

# âœ… æ­£ç¡® - æ”¯æŒçš„æ¨¡å‹
counter = TokenCounter(model_name="gpt-4")
counter = TokenCounter(model_name="claude-3")
```

### è¯­è¨€æ£€æµ‹é—®é¢˜

ä¸ºå‡†ç¡®æ£€æµ‹æä¾›è¶³å¤Ÿçš„æ–‡æœ¬ï¼š
```python
# âŒ é”™è¯¯ - å¤ªçŸ­
lang = detect_language("ä½ å¥½")  # å¯èƒ½ä¸å‡†ç¡®

# âœ… æ­£ç¡® - è¶³å¤Ÿçš„æ–‡æœ¬
lang = detect_language("ä½ å¥½ï¼Œä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ")
```

### å¯¼å…¥é”™è¯¯

ç¡®ä¿æ‚¨ä»æ­£ç¡®çš„æ¨¡å—å¯¼å…¥ï¼š
```python
# æ ¸å¿ƒå·¥å…·
from langcrew.utils.file_detect import detect_file_type
from langcrew.utils.token_counter import TokenCounter

# è¯­è¨€å·¥å…·
from langcrew.utils.language import detect_language, detect_chinese

# æ¶ˆæ¯å·¥å…·
from langcrew.utils.message_utils import generate_message_id
```

## ä¸‹ä¸€æ­¥

- **[é…ç½®æŒ‡å—](/zh/guides/utils/configuration)** - å­¦ä¹ æ‰€æœ‰é€‰é¡¹
- **[å·¥å…·åº“æ¦‚å¿µ](/zh/concepts/utils)** - ç†è§£æ¶æ„
- **ç¤ºä¾‹** - æŸ¥çœ‹ `examples/components/utils/` äº†è§£å®ç°æ¡ˆä¾‹